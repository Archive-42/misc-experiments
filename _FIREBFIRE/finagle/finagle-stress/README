com.twitter.finagle.topo

  Consists of 2 servers: Backendserver and Appserver, to be set up in
  typical application service topology. The application server exposes
  an HTTP service and fans out requests to thrift backends.

  This is the minimal toplogy for useful isolated testing and
  examination -- appserver is both a server and client (with different
  protocols), and can be configured to talk to large pools of backends,
  using a large number of connections. backend simulates latencies and
  response sizes (they are parameterized by the request). backend does
  not require configuration, request behavior is modelled completely by
  the appserver configuration, specified entirely via the command line:

  	bin/appserver port [size:milliseconds,..] [n*host:port..]

  for example

  	bin/appserver 1500 424300:88,339400:14,894300:81,695900:94 \
  	  100*localhost:2000 10*localhost:3000

  will serve http on port 1500, dispatching requests to a virtual
  cluster of two shards (served by bin/backend 2000 and bin/backend
  3000), the first which is striped over 100 finagle clients, the second
  10. Responses are sampled from the list given. It's simple to generate
  random samples:

  	jot -r 160 1000 10000 | rs 0 2 | awk '{print 100*$1 ":" int($2/100)}' | tr '\n' ','

  bin/client drives load with specified concurrency:

  	bin/client statsport targethostport concurrency

  so to drive 100 concurrent requests to the above,

  	bin/client 8000 localhost:1500 100

  where the client exports stats with ostrich on port 8000.
